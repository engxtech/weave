import { StructuredTool } from '@langchain/core/tools';
import { z } from 'zod';
import * as fs from 'fs';
import * as path from 'path';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { spawn } from 'child_process';
import { IntelligentSentenceSearch } from './intelligent-sentence-search';

interface VideoSegment {
  id: string;
  startTime: number;
  endTime: number;
  duration: number;
  relevanceScore: number;
  description: string;
  matchType: 'audio' | 'visual' | 'both';
  thumbnailPath: string;
  timestamp: string;
}

interface SearchResult {
  query: string;
  totalSegments: number;
  segments: VideoSegment[];
  processingTime: number;
}

interface TranscriptionSegment {
  id: string;
  startTime: number;
  endTime: number;
  duration: number;
  text: string;
  confidence: number;
}

interface SearchResultSegment {
  id: string;
  startTime: number;
  endTime: number;
  duration: number;
  matchType: 'audio' | 'visual' | 'both';
  relevanceScore: number;
  description: string;
  reasoning: string;
}

export class EnhancedVideoSearchTool extends StructuredTool {
  name = 'enhanced_search_video_content';
  description = 'Enhanced video search using 3-step algorithm: transcribe with timestamps, analyze with Gemini AI for visual/audio matches, return segments with thumbnails.';
  
  schema = z.object({
    query: z.string().describe('Search query (e.g., "Y combinator", "person with glasses", "startup advice")'),
    videoPath: z.string().describe('Path to video file'),
    maxResults: z.number().optional().default(5).describe('Maximum number of segments to return'),
    minRelevanceScore: z.number().optional().default(0.7).describe('Minimum relevance score (0-1) for including segments')
  });

  private genAI: GoogleGenerativeAI;

  constructor() {
    super();
    this.genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');
  }

  async _call(args: z.infer<typeof this.schema>): Promise<string> {
    const startTime = Date.now();
    
    try {
      const { query, videoPath, maxResults, minRelevanceScore } = args;
      
      console.log(`üîç Enhanced video search algorithm starting for: "${query}"`);
      console.log(`üìÅ Video path: ${videoPath}`);
      
      if (!videoPath) {
        console.log('‚ùå No video path provided for search');
        return JSON.stringify({
          query,
          totalSegments: 0,
          segments: [],
          processingTime: 0
        });
      }
      
      // USE NEW INTELLIGENT SENTENCE COMPLETION SEARCH SYSTEM
      console.log('üß† Using intelligent sentence completion search with logical audio backing...');
      const intelligentSearcher = new IntelligentSentenceSearch();
      const searchResults = await intelligentSearcher.searchVideo(videoPath, query);
      
      if (!searchResults || searchResults.length === 0) {
        console.log('‚ùå No matching segments found');
        return JSON.stringify({
          query,
          totalSegments: 0,
          segments: [],
          processingTime: Date.now() - startTime
        });
      }
      
      console.log(`üéØ Found ${searchResults.length} matches from intelligent sentence completion search`);
      
      // Results already have thumbnails generated by the intelligent search system
      const segmentsWithThumbnails = searchResults;
      
      const result = {
        query,
        totalSegments: segmentsWithThumbnails.length,
        segments: segmentsWithThumbnails,
        processingTime: Date.now() - startTime
      };
      
      console.log(`‚úÖ Enhanced search completed: ${result.totalSegments} segments with thumbnails in ${result.processingTime}ms`);
      return JSON.stringify(result);
      
    } catch (error) {
      console.error('Enhanced video search error:', error);
      return JSON.stringify({
        query: args.query,
        totalSegments: 0,
        segments: [],
        error: error instanceof Error ? error.message : 'Unknown error occurred',
        processingTime: Date.now() - startTime
      });
    }
  }

  // STEP 1: Transcribe video with sentence/time-based segmentation
  private async transcribeVideoWithTimestamps(videoPath: string): Promise<TranscriptionSegment[]> {
    return new Promise((resolve, reject) => {
      console.log('üéµ Extracting audio and detecting speech segments...');
      
      // Use FFmpeg to detect audio segments based on silence
      const ffmpegProcess = spawn('ffmpeg', [
        '-i', videoPath,
        '-af', 'silencedetect=noise=-30dB:duration=1.0',
        '-f', 'null',
        '-',
        '-nostats',
        '-loglevel', 'info'
      ]);

      const segments: TranscriptionSegment[] = [];
      let currentStart = 0;
      let segmentIndex = 0;
      
      ffmpegProcess.stderr.on('data', (data) => {
        const output = data.toString();
        
        // Parse silence detection output
        const silenceStartMatch = output.match(/silence_start: ([\d.]+)/);
        const silenceEndMatch = output.match(/silence_end: ([\d.]+)/);
        
        if (silenceStartMatch) {
          const silenceStart = parseFloat(silenceStartMatch[1]);
          
          // Create segment from current start to silence start
          if (silenceStart > currentStart && (silenceStart - currentStart) > 2) {
            segments.push({
              id: `audio_segment_${segmentIndex}`,
              startTime: currentStart,
              endTime: silenceStart,
              duration: silenceStart - currentStart,
              text: `Speech segment from ${currentStart.toFixed(1)}s to ${silenceStart.toFixed(1)}s`, // Will be transcribed by Gemini
              confidence: 0.9
            });
            segmentIndex++;
          }
        }
        
        if (silenceEndMatch) {
          currentStart = parseFloat(silenceEndMatch[1]);
        }
      });

      ffmpegProcess.on('close', async (code) => {
        // Add final segment if needed
        segments.push({
          id: `audio_segment_${segmentIndex}`,
          startTime: currentStart,
          endTime: currentStart + 10, // Default final segment
          duration: 10,
          text: `Audio segment`,
          confidence: 0.9
        });
        
        console.log(`üéµ Audio segmentation complete: ${segments.length} segments detected`);
        
        // Transcribe the entire video with Gemini for accurate speech recognition
        const actualTranscript = await this.getVideoTranscriptWithGemini(videoPath);
        if (actualTranscript) {
          console.log(`üé§ Gemini transcript: "${actualTranscript}"`);
          
          // Break transcript into logical sentences and distribute across segments
          const logicalSentences = this.breakIntoLogicalSentences(actualTranscript);
          console.log(`üìù Broken into ${logicalSentences.length} logical sentences`);
          
          // Clear existing segments and create sentence-based segments
          segments.length = 0;
          const totalDuration = 60; // Default duration, could be determined from video
          const timePerSentence = totalDuration / Math.max(logicalSentences.length, 1);
          
          logicalSentences.forEach((sentence: string, index: number) => {
            const startTime = index * timePerSentence;
            const endTime = (index + 1) * timePerSentence;
            
            segments.push({
              id: `sentence_${index}`,
              startTime: startTime,
              endTime: endTime,
              duration: timePerSentence,
              text: sentence.trim(),
              confidence: 0.95
            });
          });
          
          console.log(`‚úÖ Created ${segments.length} sentence-based segments`);
        }
        
        resolve(segments);
      });

      ffmpegProcess.on('error', (error) => {
        console.error('Audio segmentation error:', error);
        resolve([]); // Return empty array instead of rejecting
      });
    });
  }

  // Get comprehensive video analysis using Gemini AI (audio + visual)
  private async getVideoTranscriptWithGemini(videoPath: string): Promise<string> {
    try {
      const model = this.genAI.getGenerativeModel({ model: 'gemini-1.5-flash' });
      
      // Read video file for Gemini analysis
      const fullVideoPath = path.resolve('uploads', videoPath);
      const videoData = fs.readFileSync(fullVideoPath);
      
      const prompt = `
COMPREHENSIVE VIDEO CONTENT ANALYSIS

Analyze this video completely for both AUDIO and VISUAL content:

1. AUDIO TRANSCRIPTION:
   - Transcribe ALL spoken words, dialogue, and speech with precise timing
   - Include EXACT names, proper nouns, brand names, and specific terms
   - Capture background audio, music, sound effects
   - Note emotional tone and speaking style

2. VISUAL CONTENT ANALYSIS:
   - Describe people, faces, objects, and text visible on screen
   - Identify scenes, locations, environments, and settings
   - Note graphics, logos, written content, captions
   - Describe actions, movements, and visual elements

3. CONTENT UNDERSTANDING:
   - Main topics, themes, and subject matter
   - Context and purpose of the video
   - Key messages and information conveyed
   - Important moments and highlights

IMPORTANT: Return ONLY the complete audio transcription text without any additional formatting or analysis. The visual and content analysis should inform your understanding but the output should be pure transcript text that can be used for search matching.
`;

      const result = await model.generateContent([
        prompt,
        {
          inlineData: {
            data: videoData.toString('base64'),
            mimeType: 'video/mp4'
          }
        }
      ]);

      const transcript = result.response.text()?.trim();
      console.log(`üé§ Gemini comprehensive transcript: "${transcript}"`);
      return transcript || '';
    } catch (error) {
      console.error('Gemini transcription error:', error);
      return '';
    }
  }

  // STEP 2: Send video + transcription to Gemini for visual/audio search with type detection
  private async searchWithGeminiAI(
    videoPath: string, 
    transcriptionSegments: TranscriptionSegment[], 
    query: string, 
    maxResults: number
  ): Promise<SearchResultSegment[]> {
    try {
      console.log('ü§ñ Initializing Gemini model for multimodal analysis...');
      const model = this.genAI.getGenerativeModel({ model: 'gemini-1.5-flash' });
      
      // Extract representative frames for visual analysis
      const keyFrames = await this.extractKeyFramesForAnalysis(videoPath, transcriptionSegments);
      console.log(`üñºÔ∏è Extracted ${keyFrames.length} key frames for analysis`);
      
      // Prepare comprehensive prompt for Gemini
      const prompt = `
INTELLIGENT VIDEO CONTENT SEARCH

Search Query: "${query}"

VIDEO CONTENT ANALYSIS:
${transcriptionSegments.map(seg => 
  `Segment ${seg.id} (${seg.startTime.toFixed(1)}s-${seg.endTime.toFixed(1)}s):
  Spoken Text: "${seg.text}"`
).join('\n\n')}

COMPREHENSIVE VIDEO SEARCH TASK:
You are an AI video content analyst with access to both AUDIO and VISUAL content. Search through this complete video to find segments relevant to "${query}".

ANALYSIS APPROACH:
1. AUDIO ANALYSIS: Listen to the complete video audio and match against the provided transcript segments
2. VISUAL ANALYSIS: Examine the video visually for people, objects, text, scenes, and environments
3. MULTIMODAL ANALYSIS: Combine audio and visual understanding for comprehensive content matching

SEARCH CRITERIA:
1. DIRECT MATCHES:
   - Exact spoken mentions of names, terms, or phrases in audio
   - Visual text, captions, titles, or graphics showing the search term
   - People, objects, or brands visible that match the search query

2. CONTEXTUAL MATCHES:
   - Related topics, themes, or subject matter discussed
   - People, places, events, or concepts connected to the search term
   - Situations, scenes, or environments where the search term is relevant

3. SEMANTIC MATCHES:
   - Similar, synonymous, or related terms in speech or visuals
   - Content discussing the same subject matter or category
   - Implied references or contextual mentions

SEGMENT MATCHING INSTRUCTIONS:
- Use the provided logical sentence segments as timing references
- Each segment represents a meaningful speech unit with natural timing
- Match search queries against the full meaning and context of each segment
- Consider both what is spoken AND what is visible during each segment

For each relevant segment found, provide:
- Precise time range matching the logical sentence segments
- Match type: "audio" (spoken), "visual" (seen), or "both"
- Relevance score (0.0-1.0) based on match strength and context
- Detailed description with EXACT QUOTES from speech when available
- Clear reasoning explaining why this segment matches the search query

IMPORTANT REQUIREMENTS:
- Analyze the complete video content, not just transcript text
- Be thorough in examining both audio and visual elements
- Provide exact quotes from spoken content when available
- Include segments with strong contextual relevance even without exact word matches
- If genuinely no matches found, return empty segments array

RESPONSE FORMAT (JSON only):
{
  "segments": [
    {
      "startTime": 15.5,
      "endTime": 28.2,
      "matchType": "audio",
      "relevanceScore": 0.95,
      "description": "Speaker mentions: 'Rishabh Pant scored the winning runs in the final over'",
      "reasoning": "Direct audio mention of search term with relevant context"
    },
    {
      "startTime": 45.1,
      "endTime": 52.8,
      "matchType": "visual", 
      "relevanceScore": 0.87,
      "description": "Y Combinator logo visible on screen",
      "reasoning": "Visual appearance of search term in video content"
    }
  ]
}

Analyze the video content thoroughly and return segments with relevance scores above 0.7.
`;

      console.log('ü§ñ Sending comprehensive multimodal analysis request to Gemini...');
      
      // Include the full video for comprehensive audio+visual analysis
      const videoData = fs.readFileSync(path.resolve('uploads', videoPath));
      
      const parts = [
        prompt,
        {
          inlineData: {
            data: videoData.toString('base64'),
            mimeType: 'video/mp4'
          }
        }
      ];
      
      const result = await model.generateContent(parts);
      const responseText = result.response.text();
      
      console.log('ü§ñ Gemini analysis response received');
      
      // Extract JSON from response
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        console.error('No valid JSON found in Gemini response');
        return [];
      }
      
      const analysisResult = JSON.parse(jsonMatch[0]);
      
      // Convert to SearchResultSegment format
      const searchResults: SearchResultSegment[] = analysisResult.segments.map((seg: any, index: number) => ({
        id: `search_result_${index}`,
        startTime: seg.startTime,
        endTime: seg.endTime,
        duration: seg.endTime - seg.startTime,
        matchType: seg.matchType,
        relevanceScore: seg.relevanceScore,
        description: seg.description,
        reasoning: seg.reasoning
      }));
      
      console.log(`‚úÖ Gemini analysis complete: ${searchResults.length} matching segments identified`);
      searchResults.forEach(seg => {
        console.log(`  üìç ${seg.startTime}s-${seg.endTime}s (${seg.matchType}): ${seg.description}`);
      });
      
      return searchResults;
      
    } catch (error) {
      console.error('Gemini AI search error:', error);
      return [];
    }
  }

  // STEP 3: Generate thumbnails for search result segments (for list tiles)
  private async generateThumbnailsForSegments(
    videoPath: string, 
    searchResults: SearchResultSegment[]
  ): Promise<VideoSegment[]> {
    const segments: VideoSegment[] = [];
    
    console.log(`üñºÔ∏è Generating thumbnails for ${searchResults.length} segments...`);
    
    for (let i = 0; i < searchResults.length; i++) {
      const result = searchResults[i];
      
      try {
        // Generate thumbnail at segment midpoint
        const thumbnailTime = result.startTime + (result.duration / 2);
        const thumbnailPath = await this.generateThumbnail(videoPath, thumbnailTime, result.id);
        
        segments.push({
          id: result.id,
          startTime: result.startTime,
          endTime: result.endTime,
          duration: result.duration,
          relevanceScore: result.relevanceScore,
          description: result.description,
          matchType: result.matchType,
          thumbnailPath: thumbnailPath,
          timestamp: this.formatTime(result.startTime)
        });
        
        console.log(`‚úÖ Thumbnail generated for ${result.id}: ${thumbnailPath}`);
        
      } catch (error) {
        console.error(`Failed to generate thumbnail for segment ${result.id}:`, error);
        // Add segment with placeholder thumbnail
        segments.push({
          id: result.id,
          startTime: result.startTime,
          endTime: result.endTime,
          duration: result.duration,
          relevanceScore: result.relevanceScore,
          description: result.description,
          matchType: result.matchType,
          thumbnailPath: '/api/placeholder-thumbnail.jpg',
          timestamp: this.formatTime(result.startTime)
        });
      }
    }
    
    return segments;
  }

  // Helper: Extract key frames for visual analysis
  private async extractKeyFramesForAnalysis(
    videoPath: string, 
    transcriptionSegments: TranscriptionSegment[]
  ): Promise<string[]> {
    const framePaths: string[] = [];
    
    // Extract frames at the start of each audio segment for analysis
    for (const segment of transcriptionSegments.slice(0, 3)) { // Limit to first 3 for efficiency
      try {
        const frameTime = segment.startTime + 1; // 1 second into segment
        const framePath = await this.extractFrame(videoPath, frameTime, segment.id);
        framePaths.push(framePath);
      } catch (error) {
        console.error(`Failed to extract frame for segment ${segment.id}:`, error);
      }
    }
    
    return framePaths;
  }

  // Helper: Extract a single frame at specified time
  private async extractFrame(videoPath: string, timeInSeconds: number, segmentId: string): Promise<string> {
    return new Promise((resolve, reject) => {
      const outputPath = path.join('temp_frames', `frame_${segmentId}_${timeInSeconds}.jpg`);
      
      // Ensure temp directory exists
      if (!fs.existsSync('temp_frames')) {
        fs.mkdirSync('temp_frames', { recursive: true });
      }
      
      const ffmpegProcess = spawn('ffmpeg', [
        '-i', videoPath,
        '-ss', timeInSeconds.toString(),
        '-vframes', '1',
        '-y',
        outputPath
      ]);
      
      ffmpegProcess.on('close', (code) => {
        if (code === 0) {
          resolve(outputPath);
        } else {
          reject(new Error(`FFmpeg frame extraction failed with code ${code}`));
        }
      });
      
      ffmpegProcess.on('error', reject);
    });
  }

  // Helper: Generate thumbnail for list tile display
  private async generateThumbnail(videoPath: string, timeInSeconds: number, segmentId: string): Promise<string> {
    return new Promise((resolve, reject) => {
      const thumbnailFilename = `thumbnail_${segmentId}_${Date.now()}.jpg`;
      const thumbnailPath = path.join('uploads', thumbnailFilename);
      
      const ffmpegProcess = spawn('ffmpeg', [
        '-i', videoPath,
        '-ss', timeInSeconds.toString(),
        '-vframes', '1',
        '-vf', 'scale=320:180', // Standard thumbnail size for list tiles
        '-y',
        thumbnailPath
      ]);
      
      ffmpegProcess.on('close', (code) => {
        if (code === 0) {
          resolve(`/api/video/search/thumbnail/${thumbnailFilename}`);
        } else {
          reject(new Error(`Thumbnail generation failed with code ${code}`));
        }
      });
      
      ffmpegProcess.on('error', reject);
    });
  }

  // Helper: Break transcript into logical sentences with meaning-based segmentation
  private breakIntoLogicalSentences(transcript: string): string[] {
    // Advanced sentence segmentation considering speech patterns
    let sentences: string[] = [];
    
    // First split by strong sentence boundaries
    const preliminarySplit = transcript
      .split(/[.!?]+/)
      .map(s => s.trim())
      .filter(s => s.length > 3);
    
    // Further split long segments by natural speech pauses and conjunctions
    for (const segment of preliminarySplit) {
      if (segment.length > 100) {
        // Split long segments by natural breaks
        const subSegments = segment
          .split(/(?:\s+(?:and|but|so|then|now|well|okay|right|also)\s+)|(?:\s*,\s*(?:and|but|so)\s+)|(?:\s+because\s+)|(?:\s+when\s+)|(?:\s+if\s+)/)
          .map(s => s.trim())
          .filter(s => s.length > 10);
        
        sentences.push(...subSegments);
      } else if (segment.length > 5) {
        sentences.push(segment);
      }
    }
    
    // Merge very short segments with adjacent ones for better context
    const mergedSentences: string[] = [];
    for (let i = 0; i < sentences.length; i++) {
      const current = sentences[i];
      const next = sentences[i + 1];
      
      if (current.length < 20 && next && next.length < 30) {
        // Merge short adjacent segments
        mergedSentences.push(`${current} ${next}`);
        i++; // Skip the next one since we merged it
      } else {
        mergedSentences.push(current);
      }
    }
    
    console.log(`üìù Logical sentence breakdown (${mergedSentences.length} segments):`);
    mergedSentences.forEach((s, i) => {
      console.log(`   ${i+1}. "${s.substring(0, 80)}${s.length > 80 ? '...' : ''}"`);
    });
    
    return mergedSentences;
  }

  // Helper: Format time as MM:SS for display
  private formatTime(seconds: number): string {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  }
}

// Create exportable instance
export const enhancedVideoSearchTool = new EnhancedVideoSearchTool();